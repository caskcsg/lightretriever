#!/usr/bin/python
# -*- encoding: utf-8 -*-
'''
Utils for `Non-contextual query embedding`. 
It is a mean-pooled (prompted)-query eos token embedding. Query tokens are 
indivisually encoded (with optional prompt). Then query sentence embedding 
is aggregated by mean pooling the coresponding eos token ids.

@Time    :   2024/08/29
@Author  :   Ma (Ma787639046@outlook.com)
'''
import os
import numpy as np
from tqdm import tqdm
from queue import Queue
from typing import Callable, Optional
from contextlib import contextmanager

import torch
from torch import Tensor

from transformers.modeling_utils import PreTrainedModel
from transformers.tokenization_utils import PreTrainedTokenizerBase
from transformers.modeling_outputs import BaseModelOutput
from transformers.data.data_collator import pad_without_fast_tokenizer_warning

def _prepare_prefixed_attn_mask_for_nonctx_qry_tok_emb(
    attention_mask: Tensor, prompt_lengths: list[int], micro_block_width: int, is_casual: bool
):
    """ Prepare a 4-D float attention_mask from 2-D bool attention_mask. Do following process:
        1. Tokens within prompt_lengths can attend each other.
        2. Within a block of micro_block_width (these blocks are tokens after prompt), a block can only 
            attend prompt, but not attend to other blocks of micro_block_width.
        3. It handles full-attention (square-like) / casual-attention (triangle-like) by is_casual.

        So that we generate a attention_mask where each blocks of micro_block_width is indivisual,
        cannot attend each other, but share a common prompt area.

        The returned attention mask will be a 4-D float Tensor with shape [batch_size, 1, q_seq_len, kv_seq_len],
        where 0 means tokens can attend, -inf means tokens cannot attend.

        Args:
            attention_mask (Tensor): Shape [batch_size, seq_len]. A boolean mask generated by tokenizer.
            prompt_lengths (list[int]): Shape [batch_size]. A list of prompt lengths. 
            micro_block_width (int): Block width.
            is_casual (bool): Whether to process as full-attention (square-like) / casual-attention (triangle-like) mask.
    """
    batch_size, max_seq_len = attention_mask.size()
    device = attention_mask.device
    all_seq_lens = attention_mask.int().sum(-1)
    # Init all area as 1
    attn_mask_custom = torch.ones((batch_size, max_seq_len, max_seq_len), device=device)
    if is_casual:
        attn_mask_custom.tril_()

    for b in range(batch_size):
        p = prompt_lengths[b]  # Current prompt length
        curr_seq_len = all_seq_lens[b]  # Current sequence length

        # Mask out all area beyond curr_seq_len, based on lengths provided by attention_mask
        if curr_seq_len < max_seq_len:
            attn_mask_custom[b, curr_seq_len:, :] = 0.
            attn_mask_custom[b, :, curr_seq_len:] = 0.

        start = p
        while start < curr_seq_len:
            end = min(start + micro_block_width, curr_seq_len)

            # Do not attend blocks before itself
            attn_mask_custom[b, start:end, p:start] = 0.0   # p -> start

            # Do not attend blocks after itself
            if not is_casual:
                attn_mask_custom[b, start:end, end:] = 0.0
            
            start += micro_block_width

    # Convert `1 (valid), 0 (invalid) int mask` to `0 (valid), -inf (invalid) float mask`
    final_mask = torch.zeros((batch_size, max_seq_len, max_seq_len), device=device)
    final_mask.masked_fill_(~(attn_mask_custom.bool()), float("-inf"))
    final_mask = final_mask.unsqueeze(1)    # Extend dim to [batch_size, 1, seq_len, seq_len]
    return final_mask

def tokenize_nonctx_qry_tok_emb(
    queries: list[str], 
    tokenizer: PreTrainedTokenizerBase, 
    max_len: int = 512,
    prompts: Optional[list[str]] = None, 
    noncontextual_prompt_prefix: Optional[str] = None,
    is_casual: bool=False
):
    """ Tokenize a special query token embedding, with blocked attention_mask and blocked position_ids
        1. Each line of input_ids: [prompt] + [q_tok1, eos] + [q_tok2, eos] + ..., where [q_tok, eos] forms
        a micro block, share a common prompt area.
        2. Generate an attention_mask: each blocks of micro_block_width is indivisual,
        cannot attend each other, but share a common prompt area.
        3. Generate a position_ids: [0, 1, ..., prompt_len-1] + [prompt_len, prompt_len+1] + [prompt_len, prompt_len+1] + ...

        Args:
            queries (list[str]): List of string queries.
            tokenizer (PreTrainedTokenizerBase): HF Tokenizer.
            max_len (int): Max sequence length of each `query`, DO NOT include the prompt area.
            prompts (Optional[list[str]]): Optional list of prompts. Each prompt coresponds to one query.
            noncontextual_prompt_prefix (Optional[str]): Optional string prepand to each of the prompts.
            is_casual (bool): Whether to process as full-attention (square-like) / casual-attention (triangle-like) mask.
            prompt_prefix (Optional[str]): Optional string to prepend in front of each prompts.
    """
    bos_token_id = tokenizer.bos_token_id
    eos_token_id = tokenizer.eos_token_id
    # Infer bos switch
    # Fast Tokenizer does not have `add_bos_token` / `add_eos_token` attr
    # but it still adds them. We did a quick tokenize here, and infer
    # whether we should add bos or not.
    add_bos_token = bos_token_id in tokenizer.encode("", add_special_tokens=True)

    # Tokenize queries
    all_queries_tokens: list[list[int]] = tokenizer(
        queries, 
        max_length=max_len,
        truncation=True,
        add_special_tokens=False,
        return_attention_mask=False,
    )["input_ids"]

    # Combine prefix
    if prompts is not None and noncontextual_prompt_prefix is not None:
        prompts = [noncontextual_prompt_prefix + prompt_text for prompt_text in prompts]
    elif prompts is None and noncontextual_prompt_prefix is not None:
        prompts = [noncontextual_prompt_prefix] * len(queries)
    
    # Tokenize prompts
    if prompts is not None:
        all_prompt_tokens: list[list[int]] = tokenizer(
            prompts, 
            add_special_tokens=False,
            return_attention_mask=False,
        )["input_ids"]

    # Combine [Prompts] + [q_tok1, eos] + [q_tok2, eos] + ...
    batch_input_ids = []
    batch_attention_mask = []
    batch_position_ids = []
    prompt_lengths = []

    for i, query_tokens in enumerate(all_queries_tokens):
        prompt_tokens = all_prompt_tokens[i] if prompts is not None else []
        if add_bos_token:
            prompt_tokens.insert(0, bos_token_id)

        prompt_len = len(prompt_tokens)
        prompt_lengths.append(prompt_len)

        input_ids = []
        input_ids.extend(prompt_tokens)
        for q_tok_id in query_tokens:
            input_ids.append(q_tok_id)
            input_ids.append(eos_token_id)
        
        attn_mask = [True] * len(input_ids)

        pos_ids = list(range(prompt_len))
        for _ in range(len(query_tokens)):
            pos_ids.append(prompt_len)
            pos_ids.append(prompt_len + 1)
        
        batch_input_ids.append(input_ids)
        batch_attention_mask.append(attn_mask)
        batch_position_ids.append(pos_ids)
    
    padded: dict[str, Tensor] = pad_without_fast_tokenizer_warning(
        tokenizer,
        {"input_ids": batch_input_ids, "attention_mask": batch_attention_mask},
        return_tensors="pt", verbose=False,
    )
    padded_input_ids = padded["input_ids"]
    padded_attention_mask = padded["attention_mask"].bool()

    custom_attn_mask = _prepare_prefixed_attn_mask_for_nonctx_qry_tok_emb(
        padded_attention_mask, prompt_lengths, micro_block_width=2, is_casual=is_casual
    )

    # Pad position_ids
    max_len = padded_input_ids.size(1)
    padded_position_ids = torch.zeros((len(batch_position_ids), max_len), dtype=torch.long)
    for i, pos in enumerate(batch_position_ids):
        pos_tensor = torch.tensor(pos, dtype=torch.long)
        padded_position_ids[i, :pos_tensor.size(0)] = pos_tensor
    
    return {
        "input_ids": padded_input_ids,
        "attention_mask": custom_attn_mask,
        "position_ids": padded_position_ids,
        "attention_mask_2d": padded_attention_mask,
    }


def tokenize_nonctx_qry_emb_bag(
    queries: list[str], 
    tokenizer: PreTrainedTokenizerBase, 
    max_len: int = 512,
):
    """ Tokenize queries for EmbeddingBag.

        Args:
            queries (list[str]): List of string queries.
            tokenizer (PreTrainedTokenizerBase): HF Tokenizer.
            max_len (int): Max sequence length of each `query`, DO NOT include the prompt area.
    """
    encodings_ids: list[list[int]] = tokenizer(
        queries, 
        max_length=max_len,
        truncation=True,
        add_special_tokens=False,
        return_attention_mask=False,
    )["input_ids"]

    offsets = torch.from_numpy(np.cumsum([0] + [len(token_ids) for token_ids in encodings_ids[:-1]]))
    input_ids = torch.from_numpy(np.concatenate(encodings_ids)).long()
    return {"input_ids": input_ids, "offsets": offsets}


@contextmanager
def disable_tokenizer_parallelism():
    """
    Temperally set TOKENIZERS_PARALLELISM=false, so that it won't init a rayon threadpool in the main process.
    This avoid the containment of tokenizers' detection of fork status.
    """
    orig = os.environ.get("TOKENIZERS_PARALLELISM")
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    try:
        yield
    finally:
        if orig is None:
            del os.environ["TOKENIZERS_PARALLELISM"]
        else:
            os.environ["TOKENIZERS_PARALLELISM"] = orig


def construct_embedding_bag(
    model: PreTrainedModel,
    tokenizer: PreTrainedTokenizerBase,
    prompt: str = None,
    batch_size: int = 2000,
):
    """ Construct prompted token embedding bags. 
        This fuction will construct inputs formated in `[bos] + [prompts] + [vocab_token_id] + [eos]`,
        then we pool the hidden states from eos as the embedding of coresponding vocab_token_id.
        vocab_token_id ranges [0, len(tokenizer)).
    """
    model.eval()
    
    with disable_tokenizer_parallelism():
        bos_token_id = tokenizer.bos_token_id
        eos_token_id = tokenizer.eos_token_id
        pad_token_id = tokenizer.pad_token_id
        tokenizer_len = len(tokenizer)
        # Infer bos switch
        # Fast Tokenizer does not have `add_bos_token` / `add_eos_token` attr
        # but it still adds them. We did a quick tokenize here, and infer
        # whether we should add bos or not.
        add_bos_token = bos_token_id in tokenizer.encode("", add_special_tokens=True)

        # Tokenize prompts
        prompt_token_ids = []
        if prompt is not None:
            prompt_token_ids: list[int] = tokenizer.encode(
                prompt, 
                add_special_tokens=False,
            )
        if add_bos_token:
            prompt_token_ids.insert(0, bos_token_id)
        
        # Construct input tensor
        input_seq_len = len(prompt_token_ids) + 2
        input_tensor = torch.zeros([batch_size, input_seq_len], dtype=torch.long, device=model.device)

        # Fill prompt area
        if prompt_token_ids:
            # Broadcast [1, len(prompt_token_ids)] -> [batch_size, len(prompt_token_ids)]
            input_tensor[:, :len(prompt_token_ids)] = torch.tensor([prompt_token_ids], dtype=torch.long, device=model.device)
        
        # Fill eos
        input_tensor[:, -1] = eos_token_id

        # Output Embedding Tensor
        final_embedding = torch.zeros([tokenizer_len, model.config.hidden_size], dtype=torch.float32, device=model.device)

        # Loop forward
        for start_idx in tqdm(range(0, tokenizer_len, batch_size), desc=f"Construct EmbeddingBag [bs={batch_size}]"):
            end_idx = min(start_idx + batch_size, tokenizer_len)
            curr_bs = end_idx - start_idx

            # Fill token position with [start_idx: end_idx)
            input_tensor[:curr_bs, -2] = torch.arange(start_idx, end_idx, dtype=torch.long, device=model.device)

            # Forward model
            with torch.no_grad(), torch.autocast("cuda"):
                lm_out: BaseModelOutput = model(
                    input_ids=input_tensor[:curr_bs],
                    return_dict=True,
                    use_cache=False,    # Do not return `past_key_values`
                    output_hidden_states=False
                )
            
            # Fetch eos embedding, save them to output
            output_eos_embedding = lm_out.last_hidden_state[:, -1]
            final_embedding[start_idx:end_idx] = output_eos_embedding

    # Construct EmbeddingBag
    emb_bag = torch.nn.EmbeddingBag.from_pretrained(
        final_embedding, padding_idx=pad_token_id
    )
    return emb_bag


def construct_embedding_bag_parallel(
    hidden_size: int,
    tokenizer: PreTrainedTokenizerBase,
    encode_func: Callable,
    input_queue: Queue,
    output_queue: Queue,
    prompt: str = None,
    batch_size: int = 2000,
):
    """ Construct prompted token embedding bags parallelly. 
        This fuction will construct inputs formated in `[bos] + [prompts] + [vocab_token_id] + [eos]`,
        then we pool the hidden states from eos as the embedding of coresponding vocab_token_id.
        vocab_token_id ranges [0, len(tokenizer)).

        Args:
            hidden_size (int): Hidden size of LM.
            tokenizer (PreTrainedTokenizerBase): Tokenizer.
            encode_func (Callable): Main encode function to be executed. encode_func(batch: dict | BatchEncoding).
            input_queue (Queue): Send data to input queue, which receives input format (chunk_id, func, *args)
            output_queue(Queue): Fetch results from output queue, whose format is (chunk_id, rets)
            prompt (str): Prompts of embedding bag.
            batch_size (int): Encoding batch size.
    """    
    with disable_tokenizer_parallelism():
        bos_token_id = tokenizer.bos_token_id
        eos_token_id = tokenizer.eos_token_id
        pad_token_id = tokenizer.pad_token_id
        tokenizer_len = len(tokenizer)
        # Infer bos switch
        # Fast Tokenizer does not have `add_bos_token` / `add_eos_token` attr
        # but it still adds them. We did a quick tokenize here, and infer
        # whether we should add bos or not.
        add_bos_token = bos_token_id in tokenizer.encode("", add_special_tokens=True)

        # Tokenize prompts
        prompt_token_ids = []
        if prompt is not None:
            prompt_token_ids: list[int] = tokenizer.encode(
                prompt, 
                add_special_tokens=False,
            )
        if add_bos_token:
            prompt_token_ids.insert(0, bos_token_id)
        
        # Construct input tensor
        input_seq_len = len(prompt_token_ids) + 2
        input_tensor_base = torch.zeros([batch_size, input_seq_len], dtype=torch.long)

        # Fill prompt area
        if prompt_token_ids:
            # Broadcast [1, len(prompt_token_ids)] -> [batch_size, len(prompt_token_ids)]
            input_tensor_base[:, :len(prompt_token_ids)] = torch.tensor([prompt_token_ids], dtype=torch.long)
        
        # Fill eos
        input_tensor_base[:, -1] = eos_token_id

        # Output Embedding Tensor
        final_embedding = torch.zeros([tokenizer_len, hidden_size], dtype=torch.float32)

        # Loop forward
        all_start_idxs = list(range(0, tokenizer_len, batch_size))
        for start_idx in all_start_idxs:
            end_idx = min(start_idx + batch_size, tokenizer_len)
            curr_bs = end_idx - start_idx

            # Fill token position with [start_idx: end_idx)
            input_tensor = input_tensor_base[:curr_bs].clone()
            input_tensor[:, -2] = torch.arange(start_idx, end_idx, dtype=torch.long)

            # Call encode_func
            input_queue.put((start_idx, encode_func, {"input_ids": input_tensor}))
        
        # Fetch eos embedding, save them to output
        for _ in tqdm(range(len(all_start_idxs)), desc=f"Construct EmbeddingBag [bs={batch_size}]"):
            start_idx, output_eos_embedding = output_queue.get()
            end_idx = min(start_idx + batch_size, tokenizer_len)
            final_embedding[start_idx:end_idx] = output_eos_embedding

    # Construct EmbeddingBag
    emb_bag = torch.nn.EmbeddingBag.from_pretrained(
        final_embedding, padding_idx=pad_token_id
    )
    return emb_bag


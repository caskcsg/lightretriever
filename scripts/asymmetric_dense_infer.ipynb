{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e11f28",
   "metadata": {},
   "source": [
    "## Asymmetric Dense Embedding Inferencing\n",
    "\n",
    "The examples below shows how to inference with LightRetriever's asymmetric dense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951b0b3",
   "metadata": {},
   "source": [
    "### 1. Lightweight Query Encoder with a Dense EmbeddingBag\n",
    "Please first cache and save a EmbeddingBag by following `scripts/cache_emb_bag.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a285427",
   "metadata": {},
   "source": [
    "#### Load Tokenizer & EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa95f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "model_name_or_path = \"lightretriever/lightretriever-qwen2.5-1.5b\"\n",
    "emb_bag_path = \"web_search_en.emb_bag.pt\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Load EmbeddingBag\n",
    "emb_bag_weight = torch.load(emb_bag_path, map_location=\"cpu\")\n",
    "emb_bag = torch.nn.EmbeddingBag.from_pretrained(emb_bag_weight, padding_idx=tokenizer.pad_token_id)\n",
    "emb_bag = emb_bag.to(device=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f295ccb",
   "metadata": {},
   "source": [
    "#### Inference Query Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a23e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize_nonctx_qry_emb_bag(\n",
    "    queries: list[str], \n",
    "    tokenizer: PreTrainedTokenizerBase, \n",
    "    max_len: int = 512,\n",
    "):\n",
    "    \"\"\" Tokenize queries for EmbeddingBag.\n",
    "\n",
    "        Args:\n",
    "            queries (list[str]): List of string queries.\n",
    "            tokenizer (PreTrainedTokenizerBase): HF Tokenizer.\n",
    "            max_len (int): Max sequence length of each `query`, DO NOT include the prompt area.\n",
    "    \"\"\"\n",
    "    encodings_ids: list[list[int]] = tokenizer(\n",
    "        queries, \n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    offsets = torch.from_numpy(np.cumsum([0] + [len(token_ids) for token_ids in encodings_ids[:-1]]))\n",
    "    input_ids = torch.from_numpy(np.concatenate(encodings_ids)).long()\n",
    "    return {\"input\": input_ids, \"offsets\": offsets}\n",
    "\n",
    "# Tokenize\n",
    "queries: list[str] = [\n",
    "    \"How tall is Mount Everest?\",\n",
    "    \"Who invented the light bulb?\"\n",
    "]\n",
    "queries_tokenized: dict[str, torch.Tensor] = tokenize_nonctx_qry_emb_bag(queries, tokenizer=tokenizer)\n",
    "queries_tokenized = {k: v.to(device) for k, v in queries_tokenized.items()}\n",
    "\n",
    "# Encode & Pooling\n",
    "query_embeddings: torch.Tensor = emb_bag(**queries_tokenized)\n",
    "query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8e3c5",
   "metadata": {},
   "source": [
    "### 2. Full-sized LLM Document Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b6f76",
   "metadata": {},
   "source": [
    "#### Load & Merge LLM LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eec497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from peft import LoraConfig, PeftModel, LoraModel\n",
    "\n",
    "# Load Base HF Model & Peft Adapters\n",
    "config = LoraConfig.from_pretrained(model_name_or_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device,\n",
    ")\n",
    "hf_model: LoraModel = PeftModel.from_pretrained(base_model, model_name_or_path, config=config)\n",
    "hf_model = hf_model.merge_and_unload()  # Merge to single HF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa9517",
   "metadata": {},
   "source": [
    "#### Inference Corpus Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1445d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasttoken_pooling(\n",
    "    last_hidden: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "):\n",
    "    \"\"\" Last Token (EOS) Pooling\n",
    "\n",
    "        Args:\n",
    "            last_hidden (torch.Tensor): Last layer hidden states.\n",
    "            attention_mask (torch.Tensor): Attention mask.\n",
    "    \"\"\"\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "        return last_hidden[torch.arange(last_hidden.shape[0], device=last_hidden.device), last_token_indices]\n",
    "\n",
    "# Tokenize\n",
    "corpus: list[str] = [\n",
    "    \"Mount Everest is the highest mountain in the world, about 8,848 meters tall.\",\n",
    "    \"Thomas Edison invented the electric light bulb.\",\n",
    "    \"Mount Fuji is the tallest mountain in Japan.\",\n",
    "]\n",
    "corpus_tokenized = tokenizer(\n",
    "    corpus, \n",
    "    max_length=512,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "corpus_tokenized: dict[str, torch.Tensor] = {k: v.to(device) for k, v in corpus_tokenized.items()}\n",
    "\n",
    "# Encode & Pooling\n",
    "lm_out: BaseModelOutput = hf_model.model(\n",
    "    **corpus_tokenized,\n",
    "    return_dict=True,\n",
    "    use_cache=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "corpus_embedding = lasttoken_pooling(lm_out.last_hidden_state, corpus_tokenized[\"attention_mask\"])\n",
    "corpus_embedding = torch.nn.functional.normalize(corpus_embedding, p=2, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b70ece9",
   "metadata": {},
   "source": [
    "### 3. Compute Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7241ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3945, 0.0483, 0.3105],\n",
      "        [0.0076, 0.3945, 0.0148]], device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "scores = query_embeddings @ corpus_embedding.T\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "fileId": "bb7c8192-a96c-419c-94df-4335f6c1c55c",
  "filePath": "/mnt/bn/search-recall-lq-arnold/maguangyuan/internal/lightretriever/scripts/asymmetric_dense_infer.ipynb",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

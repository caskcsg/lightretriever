{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c04da38",
   "metadata": {},
   "source": [
    "## Asymmetric Sparse Embedding Inferencing\n",
    "\n",
    "The examples below shows how to inference with LightRetriever's asymmetric sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74a462",
   "metadata": {},
   "source": [
    "### 1. Lightweight Query Encoder without using any LM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
    "\n",
    "model_name_or_path = \"lightretriever/lightretriever-qwen2.5-1.5b\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Tokenize\n",
    "queries: list[str] = [\n",
    "    \"How tall is Mount Everest?\",\n",
    "    \"Who invented the light bulb?\"\n",
    "]\n",
    "query_input_ids: list[list[int]] = tokenizer(\n",
    "    queries, \n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    "    return_attention_mask=False,\n",
    ")[\"input_ids\"]\n",
    "\n",
    "# Get Query Embedding via counting\n",
    "query_embeddings: list[dict[int, int]] = [dict(Counter(input_ids)) for input_ids in query_input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f97fabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{5158: 1, 16217: 1, 374: 1, 6470: 1, 3512: 1, 477: 1, 30: 1}, {14623: 1, 35492: 1, 279: 1, 3100: 1, 45812: 1, 30: 1}]\n"
     ]
    }
   ],
   "source": [
    "print(query_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1282e0",
   "metadata": {},
   "source": [
    "### 2. Full-sized LLM Document Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a6f74",
   "metadata": {},
   "source": [
    "#### Load & Merge LLM LoRA Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde595e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, PeftModel, LoraModel\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Load Base HF Model & Peft Adapters\n",
    "config = LoraConfig.from_pretrained(model_name_or_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=device,\n",
    ")\n",
    "hf_model: LoraModel = PeftModel.from_pretrained(base_model, model_name_or_path, config=config)\n",
    "hf_model = hf_model.merge_and_unload()  # Merge to single HF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9905d",
   "metadata": {},
   "source": [
    "#### Inference Corpus Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4f6d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "# Tokenize\n",
    "corpus: list[str] = [\n",
    "    \"Mount Everest is the highest mountain in the world, about 8,848 meters tall.\",\n",
    "    \"Thomas Edison invented the electric light bulb.\",\n",
    "    \"Mount Fuji is the tallest mountain in Japan.\",\n",
    "]\n",
    "corpus_tokenized = tokenizer(\n",
    "    corpus, \n",
    "    max_length=512,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "corpus_tokenized: dict[str, torch.Tensor] = {k: v.to(device) for k, v in corpus_tokenized.items()}\n",
    "\n",
    "# Encode & Pooling\n",
    "lm_out: CausalLMOutputWithPast = hf_model(\n",
    "    **corpus_tokenized,\n",
    "    return_dict=True,\n",
    "    use_cache=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "lm_out.logits = lm_out.logits.masked_fill(\n",
    "    ~corpus_tokenized[\"attention_mask\"].bool().unsqueeze(-1), \n",
    "    0.\n",
    ")\n",
    "aggregated_logits: torch.Tensor = torch.log1p(torch.relu(torch.amax(lm_out.logits, dim=1)))\n",
    "\n",
    "corpus_embeddings: list[dict[int, float]] = []\n",
    "for logits in aggregated_logits:\n",
    "    idx = torch.nonzero(logits, as_tuple=False).squeeze(1)\n",
    "    vals = logits[idx]\n",
    "    emb = dict(zip(idx.tolist(), vals.tolist()))\n",
    "    corpus_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534b1180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2261, 2207, 2651]\n"
     ]
    }
   ],
   "source": [
    "print([len(emb) for emb in corpus_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e6a88",
   "metadata": {},
   "source": [
    "### 3. Compute Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45bb676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.94921875, 1.28125, 9.1875], [1.76953125, 11.74609375, 1.81640625]]\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity(\n",
    "    q_rep: dict[int, int],\n",
    "    p_rep: dict[int, float],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the similarity between two sparse vectors represented as dictionaries.\n",
    "\n",
    "    Each dictionary maps a token_id (int) to its learned pseudo-frequency (integer).\n",
    "    The similarity is defined as the dot product over the intersection of keys:\n",
    "        sum(q_rep[tok_id] * p_rep[tok_id] for tok_id in q_rep.keys() & p_rep.keys())\n",
    "\n",
    "    Args:\n",
    "        q_rep (dict[int, int]): Sparse representation of the query vector.\n",
    "        p_rep (dict[int, float]): Sparse representation of the passage/document vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score (dot product).\n",
    "    \"\"\"\n",
    "    return sum(\n",
    "        q_rep[tok_id] * p_rep[tok_id] for tok_id in q_rep.keys() & p_rep.keys()\n",
    "    )\n",
    "\n",
    "\n",
    "scores = [\n",
    "    [compute_similarity(q_rep, p_rep) for p_rep in corpus_embeddings] for q_rep in query_embeddings\n",
    "]\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
